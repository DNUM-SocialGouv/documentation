# üß™ Tests de performance

Ces tests permettent de v√©rifier la performance des fonctionnalit√©s d'une application.

Une mont√©e en charge est simul√©e pour s'assurer que le temps de r√©ponse reste acceptable.

Il est courant de mandater un prestataire pour la r√©alisation de ces tests. L'√©quipe d√©termine alors les cas √† tester et les conditions de r√©ussite des tests.

Il est tout √† fait possible pour l'√©quipe produit d'ex√©cuter certains tests en autonomie, ce qui permet d'it√©rer plus rapidement.

Dans le cas o√π le produit num√©rique est infog√©r√©, la collaboration avec l'infog√©rant est indispensable pour s'assurer de tests repr√©sentatifs et pour ne pas perturber l'infrastructure.

De plus, ces tests permettent de v√©rifier la r√©silience de l'application et l'infrastructure : scalabilit√©, redondance, etc.

## Cas √† tester

Il est recommand√© de lister d'abord les cas les plus importants √† tester, tels que :

* Les parcours utilisateurs critiques (qui ne doivent pas √©chouer).
* Les parcours √† contrainte forte de performance (ex : r√©ponse en moins de _x_ secondes).
* Les parcours susceptibles d'√™tre utilis√©s par un nombre croissant d'utilisateurs (ex : mont√©e en charge pr√©vue).
* √Ä l'inverse, tester tous les cas n'est pas un gage de qualit√©.

Un √©change avec le [responsable m√©tier](../../preparer-et-lancer/les-differents-roles-et-metiers/responsable-metier.md) et/ou le [responsable produit](../../preparer-et-lancer/les-differents-roles-et-metiers/responsable-produit.md) permet d'identifier les cas pertinents.

## Consid√©rations importantes

* Prendre en compte la richesse du r√©seau : ne pas tester √† partir du r√©seau interne qui ne simulerait pas les conditions r√©elles (WAF, r√©seaux priv√©s, passerelles, gateways, etc.)
* Tester en environnement iso-production : les utilisateurs √©tant en production, pour √™tre s√ªr de fournir un service adapt√©, ex√©cuter ces tests sur un environnement proche. Ne pas uniquement se baser sur des tests ex√©cut√©s sur les postes de d√©veloppement ou depuis les serveurs d'une cha√Æne de CI.
* √âviter les sous-estimations : tester avec les conditions pr√©vues et tester avec des conditions plus pouss√©es.
* Identifier les syst√®mes impact√©s par les tests : en cas de communication avec des APIs externes, informer les acteurs et prendre des actions adapt√©es si n√©cessaire (ex : bouchons).

## Quand faire appel √† une prestation externe

Bien qu'une prestation puisse √™tre on√©reuse, elle est pr√©f√©rable suivant le contexte :

* **Les cas pertinents √† tester ne sont pas encore identifi√©s** et une expertise externe peut aider √† les d√©finir.
* **L'√©quipe n'a pas le temps** : une expertise externe permet d'aider la r√©daction et l'ex√©cution des tests.
* **L'√©quipe n'a pas les comp√©tences** ni le temps de les acqu√©rir : une expertise externe permet de d√©marrer en √©vitant les √©cueils et de monter en comp√©tence.
* **L'√©quipe n'a pas la maitrise de l'h√©bergement** : l'infog√©rant pourrait prendre en charge une partie des actions.

A noter que toute sollicitation externe (contractualisation, mobilisation de l'expert, prise de connaissance, etc.) allonge la boucle de feedback par rapport √† une gestion interne par l'√©quipe

### Outils sur le march√©

Certains prestataires s'en remettent √† des solutions propri√©taires ou des tests configur√©s manuellement.\
Voici des solutions alternatives open-source :

* [**K6 (notre recommandation)**](https://k6.io/) : propose une API JS/TS.
* [Gatling](https://docs.gatling.io/) : propose une API JS/TS et Java.
* [Artillery](https://github.com/artilleryio/artillery) : propose une API YAML, et s'int√®gre avec Playwright.
* [JMeter](https://jmeter.apache.org/) : propose une API Java.
